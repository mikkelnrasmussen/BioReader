{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the use of a pre-trained transformer model for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data wrangling\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyprojroot.here import here\n",
    "\n",
    "# Model training\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data and combine\n",
    "data_path = here(\"data/training_data\")\n",
    "all_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith(\".csv\")]\n",
    "df_list = [pd.read_csv(file) for file in all_files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Load the excel file containing category information\n",
    "category_info = pd.read_excel(here(\"data/All_Updated_Categories_2019_edited_by_mikkel.xlsx\"))\n",
    "\n",
    "# Merge the dataframes\n",
    "merged_df = pd.merge(\n",
    "  combined_df, \n",
    "  category_info, \n",
    "  left_on = \"SubType\", \n",
    "  right_on = \"subcategory\",\n",
    "  how = \"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts:\n",
      " class\n",
      "Infectious_Disease    27508\n",
      "Other                 11514\n",
      "Autoimm               11167\n",
      "Cancer                10957\n",
      "Allergen               4019\n",
      "Transplant             1805\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Subcategory Counts:\n",
      " subcategory\n",
      "OTC       2885\n",
      "OTFLU     2534\n",
      "OTGA      2479\n",
      "PLASMO    2183\n",
      "SARS      2175\n",
      "          ... \n",
      "FIB         23\n",
      "METAL       23\n",
      "BIME        20\n",
      "INTG        17\n",
      "PROST       16\n",
      "Name: count, Length: 176, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the number of documents for each class, category, and subcategory\n",
    "class_counts = merged_df['class'].value_counts()\n",
    "subcategory_counts = merged_df['subcategory'].value_counts()\n",
    "\n",
    "print(\"Class Counts:\\n\", class_counts)\n",
    "print(\"\\nSubcategory Counts:\\n\", subcategory_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " PubMed_ID           0\n",
      "Title               8\n",
      "Abstract            0\n",
      "SubType             0\n",
      "Class             487\n",
      "Category        16265\n",
      "Subcategory       487\n",
      "Abbreviation      487\n",
      "OK                487\n",
      "class             487\n",
      "category        16265\n",
      "subcategory       487\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = merged_df.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PubMed_ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>SubType</th>\n",
       "      <th>Class</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subcategory</th>\n",
       "      <th>Abbreviation</th>\n",
       "      <th>OK</th>\n",
       "      <th>class</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5459</th>\n",
       "      <td>11006009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Data extracted from this article was imported...</td>\n",
       "      <td>HCV</td>\n",
       "      <td>Infectious Disease</td>\n",
       "      <td>ssRNA (+) Strand Virus</td>\n",
       "      <td>Hepatitis C Virus</td>\n",
       "      <td>HCV</td>\n",
       "      <td>x</td>\n",
       "      <td>Infectious_Disease</td>\n",
       "      <td>ssRNA_positive</td>\n",
       "      <td>HCV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>8567982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Data extracted from this article was imported...</td>\n",
       "      <td>HBV</td>\n",
       "      <td>Infectious Disease</td>\n",
       "      <td>Retro-Transcribing Virus</td>\n",
       "      <td>Hepatitis B Virus</td>\n",
       "      <td>HBV</td>\n",
       "      <td>x</td>\n",
       "      <td>Infectious_Disease</td>\n",
       "      <td>Retro-Transcribing_Virus</td>\n",
       "      <td>HBV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13215</th>\n",
       "      <td>11012976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Data extracted from this article was imported...</td>\n",
       "      <td>OTFLU</td>\n",
       "      <td>Infectious Disease</td>\n",
       "      <td>ssRNA (-) Strand Virus</td>\n",
       "      <td>Other Influenza A Subtypes</td>\n",
       "      <td>OTFLU</td>\n",
       "      <td>x</td>\n",
       "      <td>Infectious_Disease</td>\n",
       "      <td>ssRNA_negative</td>\n",
       "      <td>OTFLU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16750</th>\n",
       "      <td>11426965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Data extracted from this article was imported...</td>\n",
       "      <td>HPV</td>\n",
       "      <td>Infectious Disease</td>\n",
       "      <td>dsDNA Virus</td>\n",
       "      <td>Human papillomavirus</td>\n",
       "      <td>HPV</td>\n",
       "      <td>x</td>\n",
       "      <td>Infectious_Disease</td>\n",
       "      <td>dsDNA_Virus</td>\n",
       "      <td>HPV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24876</th>\n",
       "      <td>15585860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Structural and physiological facets of carbohy...</td>\n",
       "      <td>MOAB</td>\n",
       "      <td>Other</td>\n",
       "      <td>Peptidic Antigen</td>\n",
       "      <td>General Monoclonal Antibodies</td>\n",
       "      <td>MOAB</td>\n",
       "      <td>x</td>\n",
       "      <td>Other</td>\n",
       "      <td>Peptidic_Antigen</td>\n",
       "      <td>MOAB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44549</th>\n",
       "      <td>15001714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CD8 T lymphocytes recognize peptides of 8 to 1...</td>\n",
       "      <td>MAA</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tyrosinase, TRP2, GP100, TRP1, MART1, SOX10 (M...</td>\n",
       "      <td>MAA</td>\n",
       "      <td>x</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54471</th>\n",
       "      <td>29572442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The B cell survival factor (TNFSF13B/BAFF) is ...</td>\n",
       "      <td>OTLUP</td>\n",
       "      <td>Autoimmune</td>\n",
       "      <td>Lupus</td>\n",
       "      <td>Other</td>\n",
       "      <td>OTLUP</td>\n",
       "      <td>x</td>\n",
       "      <td>Autoimm</td>\n",
       "      <td>Lupus</td>\n",
       "      <td>OTLUP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63300</th>\n",
       "      <td>14724640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cytotoxic T lymphocytes (CTLs) detect and dest...</td>\n",
       "      <td>RENAL</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Renal (RCC)</td>\n",
       "      <td>RENAL</td>\n",
       "      <td>x</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RENAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PubMed_ID Title                                           Abstract  \\\n",
       "5459    11006009   NaN  [Data extracted from this article was imported...   \n",
       "10499    8567982   NaN  [Data extracted from this article was imported...   \n",
       "13215   11012976   NaN  [Data extracted from this article was imported...   \n",
       "16750   11426965   NaN  [Data extracted from this article was imported...   \n",
       "24876   15585860   NaN  Structural and physiological facets of carbohy...   \n",
       "44549   15001714   NaN  CD8 T lymphocytes recognize peptides of 8 to 1...   \n",
       "54471   29572442   NaN  The B cell survival factor (TNFSF13B/BAFF) is ...   \n",
       "63300   14724640   NaN  Cytotoxic T lymphocytes (CTLs) detect and dest...   \n",
       "\n",
       "      SubType               Class                  Category  \\\n",
       "5459      HCV  Infectious Disease    ssRNA (+) Strand Virus   \n",
       "10499     HBV  Infectious Disease  Retro-Transcribing Virus   \n",
       "13215   OTFLU  Infectious Disease    ssRNA (-) Strand Virus   \n",
       "16750     HPV  Infectious Disease               dsDNA Virus   \n",
       "24876    MOAB               Other          Peptidic Antigen   \n",
       "44549     MAA              Cancer                       NaN   \n",
       "54471   OTLUP          Autoimmune                     Lupus   \n",
       "63300   RENAL              Cancer                       NaN   \n",
       "\n",
       "                                             Subcategory Abbreviation OK  \\\n",
       "5459                                   Hepatitis C Virus          HCV  x   \n",
       "10499                                  Hepatitis B Virus          HBV  x   \n",
       "13215                         Other Influenza A Subtypes        OTFLU  x   \n",
       "16750                               Human papillomavirus          HPV  x   \n",
       "24876                      General Monoclonal Antibodies         MOAB  x   \n",
       "44549  Tyrosinase, TRP2, GP100, TRP1, MART1, SOX10 (M...          MAA  x   \n",
       "54471                                              Other        OTLUP  x   \n",
       "63300                                        Renal (RCC)        RENAL  x   \n",
       "\n",
       "                    class                  category subcategory  \n",
       "5459   Infectious_Disease            ssRNA_positive         HCV  \n",
       "10499  Infectious_Disease  Retro-Transcribing_Virus         HBV  \n",
       "13215  Infectious_Disease            ssRNA_negative       OTFLU  \n",
       "16750  Infectious_Disease               dsDNA_Virus         HPV  \n",
       "24876               Other          Peptidic_Antigen        MOAB  \n",
       "44549              Cancer                       NaN         MAA  \n",
       "54471             Autoimm                     Lupus       OTLUP  \n",
       "63300              Cancer                       NaN       RENAL  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the papers with missing titles\n",
    "merged_df[merged_df.Title.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there are some of the abstract there are not extracted correctly.\n",
    "They all starts with \"[Data extracted from this article was imported...\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before filtering: 67457\n",
      "Number of rows after filtering: 67403\n",
      "Difference: 54\n"
     ]
    }
   ],
   "source": [
    "# Create a boolean mask for rows where the abstract starts with the specified string\n",
    "mask = merged_df['Abstract'].str.startswith(\"[Data extracted from this article was imported\")\n",
    "\n",
    "# Filter out these rows\n",
    "filtered_df = merged_df[~mask]\n",
    "\n",
    "# Check the number of rows before and after filtering\n",
    "print(f\"Number of rows before filtering: {len(merged_df)}\")\n",
    "print(f\"Number of rows after filtering: {len(filtered_df)}\")\n",
    "print(f\"Difference: {len(merged_df) - len(filtered_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " PubMed_ID           0\n",
      "Title               4\n",
      "Abstract            0\n",
      "SubType             0\n",
      "Class             487\n",
      "Category        16265\n",
      "Subcategory       487\n",
      "Abbreviation      487\n",
      "OK                487\n",
      "class             487\n",
      "category        16265\n",
      "subcategory       487\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = filtered_df.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after dropping NAs in 'class': 66916\n",
      "Number of rows in train dataset: 46841\n",
      "Number of rows in validation dataset: 10037\n",
      "Number of rows in test dataset: 10038\n"
     ]
    }
   ],
   "source": [
    "### Split the Data for the class variable\n",
    "\n",
    "# Drop rows where 'class' is NA\n",
    "filtered_df = filtered_df.dropna(subset=['class'])\n",
    "\n",
    "# Check the number of rows after dropping\n",
    "print(f\"Number of rows after dropping NAs in 'class': {len(filtered_df)}\")\n",
    "\n",
    "# Split the data into train and a temporary dataset (70% train, 30% temp)\n",
    "train_df, temp_df = train_test_split(filtered_df, test_size=0.3, random_state=42, stratify=filtered_df['class'])\n",
    "\n",
    "# Split the temporary dataset into validation and test datasets (50% validation, 50% test from the temp dataset)\n",
    "eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['class'])\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Transform class label from string to integers\n",
    "# Fit the encoder on the 'class' column and transform it\n",
    "train_df['class_int'] = label_encoder.fit_transform(train_df['class'])\n",
    "eval_df['class_int'] = label_encoder.transform(eval_df['class'])\n",
    "test_df['class_int'] = label_encoder.transform(test_df['class'])\n",
    "\n",
    "\n",
    "# Check the number of rows in each dataset\n",
    "print(f\"Number of rows in train dataset: {len(train_df)}\")\n",
    "print(f\"Number of rows in validation dataset: {len(eval_df)}\")\n",
    "print(f\"Number of rows in test dataset: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents from each class in train dataset:\n",
      " class\n",
      "Infectious_Disease    19219\n",
      "Other                  8058\n",
      "Autoimm                7817\n",
      "Cancer                 7670\n",
      "Allergen               2813\n",
      "Transplant             1264\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of documents from each class in validation dataset:\n",
      " class\n",
      "Infectious_Disease    4118\n",
      "Other                 1727\n",
      "Autoimm               1675\n",
      "Cancer                1643\n",
      "Allergen               603\n",
      "Transplant             271\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of documents from each class in test dataset:\n",
      " class\n",
      "Infectious_Disease    4119\n",
      "Other                 1727\n",
      "Autoimm               1675\n",
      "Cancer                1644\n",
      "Allergen               603\n",
      "Transplant             270\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the number of documents from each class in the train dataset\n",
    "train_class_counts = train_df['class'].value_counts()\n",
    "\n",
    "# Check the number of documents from each class in the validation dataset\n",
    "valid_class_counts = eval_df['class'].value_counts()\n",
    "\n",
    "# Check the number of documents from each class in the test dataset\n",
    "test_class_counts = test_df['class'].value_counts()\n",
    "\n",
    "print(\"Number of documents from each class in train dataset:\\n\", train_class_counts)\n",
    "print(\"\\nNumber of documents from each class in validation dataset:\\n\", valid_class_counts)\n",
    "print(\"\\nNumber of documents from each class in test dataset:\\n\", test_class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the BioBert transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BioBERT and tokenizer\n",
    "model_name = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "num_labels = len(train_df['class_int'].unique())\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# 1. Tokenization\n",
    "data_size = 100\n",
    "train_abstract = train_df['Abstract'].tolist()[1:data_size]\n",
    "eval_abstract = eval_df['Abstract'].tolist()[1:data_size]\n",
    "tokenized_train = tokenizer(train_abstract, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "tokenized_eval = tokenizer(eval_abstract, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Convert tokenized data to PyTorch dataset\n",
    "train_labels = train_df['class_int'].tolist()[1:data_size]\n",
    "eval_labels = eval_df[\"class_int\"].tolist()[1:data_size]\n",
    "train_dataset = CustomDataset(encodings = tokenized_train, labels = train_labels)\n",
    "eval_dataset = CustomDataset(encodings = tokenized_eval, labels = eval_labels)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 6/13 [00:45<00:53,  7.65s/it]\n",
      "  8%|▊         | 1/13 [00:07<01:30,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1.2752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 2/13 [00:15<01:25,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1.7707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3/13 [00:23<01:16,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1.2237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 4/13 [00:29<01:06,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1.3935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 5/13 [00:37<00:59,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1.8186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 6/13 [00:44<00:51,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1.6766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 7/13 [00:52<00:44,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1.5255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 8/13 [00:59<00:36,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1.6928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 9/13 [01:06<00:29,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1.4806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 10/13 [01:14<00:22,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1.5948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 11/13 [01:21<00:14,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1.4730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 12/13 [01:29<00:07,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1.5176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:31<00:00,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1.5683\n",
      "Epoch 1 Average Loss: 1.5393\n"
     ]
    }
   ],
   "source": [
    "# 2. Training Loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", \n",
    "    optimizer=optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "\n",
    "# For accumulating loss over the epoch\n",
    "total_loss = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Add the batch loss to the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        # Print loss for the current batch\n",
    "        print(f\"Batch Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Reset total_loss for the next epoch\n",
    "    total_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5353535353535354}\n"
     ]
    }
   ],
   "source": [
    "# 3. Evaluation\n",
    "metric = load_metric(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
